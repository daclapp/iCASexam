
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ISLR, tree, MASS, randomForest, gbm)
source("load_packages.R") #load these last so overwrite functions other packages (like dplyr::select)

 #for data used in book  https://cran.r-project.org/web/packages/ISLR/ISLR.pdf
#ISLR::  #type this and it will autofill all datasets in this package
#library(tree) #requires R 3.6.0 or higher

```

Solutions from students here: https://github.com/asadoughi/stat-learning
course based on this book: https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about

Also see ISL labs.R in this repo
Also https://www.alsharif.info/iom530 for more examples

Chapter 8: Classification Trees
```{r}
Carseats = ISLR::Carseats

Carseats = Carseats %>% 
  mutate(High = if_else(Sales <=8, "No","Yes"))

Carseats
```


```{r}
Carseats_tree = tree(High ~ . -Sales, Carseats)
# summary(Carseats_tree) #causes error. tree package changed?
# plot(Carseats_tree)
```

Re-write Chapter 8 Lab for regression tree:
```{r}
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```

```{r}
set.seed(1)
boston = MASS::Boston %>% 
  mutate(split = as.factor(sample(c("test","train"), nrow(Boston), replace=T, prob = c(0.5,0.5))))
#or fold = ntile(sample(1:1000,nrow(combined2),replace=T),5)) for cross validation to guarentee equal bucketrs. Could also use for above just have to replace ntile with something that gets x percentile

boston

train = boston %>% filter(split == "train") # can't do directly in tree() call below or cv.tree will throw an error

```

```{r}

boston_tree = tree(medv ~ . ,train)
summary(boston_tree)

plot(boston_tree)
text(boston_tree,pretty=0)

boston_cv = cv.tree(boston_tree)
plot(cv.boston$size,cv.boston$dev,type='b') #online cource had ,FUN = prune.misclass on carseats data
boston_tree_prune = prune.tree(boston_tree,best=5)


plot(boston_tree_prune)
text(boston_tree_prune,pretty=0)


boston_preds = boston %>% 
  mutate(yhat = predict(boston_tree_prune,newdata = boston )) %>% 
  filter(split == "test") 


plot(boston_preds$yhat,boston_preds$medv)
abline(0,1)
 
# boston_preds %>% 
#   ggplot(aes(yhat,medv))+
#   geom_point()+
#   geom_abline()

print(paste0("RMSE: ",sqrt(mean((boston_preds$yhat-boston_preds$medv)^2)))) #on same scale as orig data, why not always use this instead of two below?
print(paste0("MSE: ",mean((boston_preds$yhat-boston_preds$medv)^2)))
print(paste0("SSE: ",sum((boston_preds$yhat-boston_preds$medv)^2)))


```


```{r}
# Bagging and Random Forests

boston_bag = randomForest(medv ~ . ,data = train,mtry = 13, importance = T)
boston_bag

boston_preds = boston %>% 
  mutate(yhat = predict(boston_bag,newdata = boston )) %>% 
  filter(split == "test") 
# importance(boston_bag)
varImpPlot(boston_bag)

plot(boston_preds$yhat,boston_preds$medv)
abline(0,1)


print(paste0("RMSE: ",sqrt(mean((boston_preds$yhat-boston_preds$medv)^2)))) #on same scale as orig data, why not always use this instead of two below?
print(paste0("MSE: ",mean((boston_preds$yhat-boston_preds$medv)^2)))
print(paste0("SSE: ",sum((boston_preds$yhat-boston_preds$medv)^2)))


```

boosting page 330

Boosting
--------
Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble.
```{r}
pacman::p_load(gbm)


boston_boost = gbm(medv ~ . ,
                   data=train,
                   distribution="gaussian",
                   n.trees=10000,
                   shrinkage=0.01,
                   interaction.depth=4)
# distribution="gaussian" for regression
# distribution="bernoulli" for binary classification

summary(boston_boost)

#partial dependence plots. marginal effect of the var on response after integrating out the other variables. 
plot(boston_boost,i="lstat")
plot(boston_boost,i="rm")


```
"Lets make a prediction on the test set. With boosting, the number of trees is a tuning parameter, and if we have too many we can overfit. So we should use cross-validation to select the number of trees. We will leave this as an exercise. Instead, we will compute the test error as a function of the number of trees, and make a plot."

```{r}

#rewrite as for loop of n.trees, then gather those columns to plot. 
predmat = predict(boston_boost,
                  newdata = boston %>% filter(split == "test"), 
                  n.trees = seq(from=100,to=10000,by=100))

test_num_trees = boston %>%
  filter(split == "test") %>% 
  select(medv) %>% 
  bind_cols(as_tibble(predmat)) %>% 
  gather(key = "num_trees", value = "yhat", -medv) %>% 
  group_by(num_trees) %>% 
  summarize(MSE = mean((yhat-medv)^2))

plot(test_num_trees$num_trees,test_num_trees$MSE)


#make prediction and calc MSE

boston_preds = boston %>% 
  mutate(yhat = predict(boston_boost,newdata = boston ,n.trees = 6000)) %>% 
  filter(split == "test") 


print(paste0("RMSE: ",sqrt(mean((boston_preds$yhat-boston_preds$medv)^2)))) #on same scale as orig data, why not always use this instead of two below?
print(paste0("MSE: ",mean((boston_preds$yhat-boston_preds$medv)^2)))
print(paste0("SSE: ",sum((boston_preds$yhat-boston_preds$medv)^2)))
```







Chapter 8: Regression Trees
```{r}
Hitters = ISLR::Hitters
# Hitters = Hitters %>% 
#   mutate(log_salary = log(Salary))

Hitters
```

Figure 8.1
```{r}


Hitters_tree = tree(log(Salary) ~ Hits + Years ,Hitters)#,subset=train)
summary(Hitters_tree)

plot(Hitters_tree)
text(Hitters_tree) #,pretty=0)  #doesn't seem to do anything
#did they change a parameter to get less leafs?

prune.tree(Hitters_tree)
```


```{r}
cv.Hitters=cv.tree(tree.Hitters)
plot(cv.Hitters$size,cv.Hitters$dev,type='b')
prune.Hitters=prune.tree(tree.Hitters,best=5)

plot(prune.Hitters)
text(prune.Hitters,pretty=0)

yhat=exp(predict(tree.Hitters,newdata=Hitters[-train,]))
set.seed(1)
train = sample(1:nrow(Hitters), nrow(Hitters)/2)
Hitters.test=Hitters[-train,"Salary"] #not working
plot(yhat,Hitters.test)
abline(0,1)
mean((yhat-Hitters.test)^2)
```


```{r}
```



```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ISLR,boot)
source("load_packages.R") #load these last so overwrite functions other packages (like dplyr::select)

```

5.3.1 validation set approach
```{r}
set.seed(1)
train_row_nums = sample(1:392,196) # same as sample(392,196), but this is more cohesive when it is used like sample(c("Train","Test"),196,replace=T)
train_row_nums

#Not sure why this data has 397 rows and textbook says it had 392
```

```{r}
auto = ISLR::Auto

lm_fit = lm(mpg ~ horsepower,
            data = auto,
            subset = train_row_nums)

#calc MSE on test:
auto_preds = auto %>% 
  mutate(mpg_pred = predict(lm_fit, newdata = auto ),
         SE = (mpg - mpg_pred)^2) %>% 
    slice(-train_row_nums) #do last so don't have to slice inside predict() too

mean(auto_preds$SE)

# mean((auto$mpg - predict(lm_fit,newdata = auto))[-train]^2) book method, I get same answer here, but doesn't match book answer
```

```{r}
lm_fit_2 = lm(mpg ~ poly(horsepower,2),
              data = auto,
              subset = train_row_nums)

mean((auto$mpg - predict(lm_fit_2,newdata = auto))[-train_row_nums]^2) 
              
lm_fit_3 = lm(mpg ~ poly(horsepower,3),
              data = auto,
              subset = train_row_nums)

mean((auto$mpg - predict(lm_fit_3,newdata = auto))[-train_row_nums]^2) 
          
```

5.3.2 Leave-One-Out Cross-Validation
```{r}
glm_fit = glm(mpg ~ horsepower,
              data = auto) #no family so fits a lm()
cv_error = cv.glm(auto,glm_fit)
cv_error$delta #cross validation results. same as mean of MSE of loocv
```


```{r}
cv_error = rep(0,5)
for (i in 1:5){ #testing 5 degrees of polynomial using loocv. not 5 fold cv
  glm_fit = glm(mpg ~ poly(horsepower,i),
                data = auto)
  cv_error[i] = cv.glm(auto,glm_fit)$delta[1] #delta 1 and 2 basically the same for loocv
}
cv_error
plot(cv_error)
```

k-fold cross validation
```{r}

#only diff from loocv above is k=10 in cv.glm

set.seed(17)
cv_error_5 = rep(0,5)
for (i in 1:5){
 glm_fit = glm(mpg ~ poly(horsepower,i),
               data = auto)
 cv_error_5[i] = cv.glm(auto,glm_fit,K=10)$delta[1] #first delta is standard cv estimate, second is bias corrected version. ?cv.glm will tell you this
 
}
plot(cv_error_5)
```

5.3.4 learning objective C.2
Bootstrap
```{r}

portfolio = ISLR::Portfolio
#step 1: create function that computes statistic of interest
alpha_fun = function(data,index){
  X = data$X[index]
  Y = data$Y[index]
  return(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)) #returns estimate for alpha on page 187. alpha minimizes total variance in investment
}

alpha_fun(portfolio,1:100)
#step 2: use boot() to sample with replacement
set.seed(1)
alpha_fun(portfolio,sample(x = 100, size = 100, replace = T)) #boot() function automates this line many times and cals sd of samples alphas. would be easy to write in for loop

pacman::p_load(boot)
boot(portfolio,alpha_fun,R=1000) #not sure why this doesn't match book

```

```{r}
auto = ISLR::Auto

boot_fun = function(data,index)
return(coef(lm(mpg~horsepower,data = data, subset = index)))

boot_fun(auto,1:392) #parameters using all data

set.seed(1)
boot_fun(auto,sample(392,392,replace=T))

#SE using bootstrap:
boot(auto,boot_fun,1000) 

#SE using formulas:
summary(lm(mpg~horsepower,data = auto))$coef #relies on assumptions that bootstrap is not tied to


```

